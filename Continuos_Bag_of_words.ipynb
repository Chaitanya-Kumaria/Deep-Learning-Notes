{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPS/miLXsjI6PclxPgVH2Z0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Chaitanya-Kumaria/Deep-Learning-Notes/blob/main/Continuos_Bag_of_words.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Continuos Bag Of Words Model"
      ],
      "metadata": {
        "id": "cDZ1UgvXz1MU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, we define a toy training dataset consisting of three sentences: \"i love you\", \"you are amazing\", and \"i hate you\". We set the vocabulary size to 6 and the embedding dimension to 3. We also set the context window size to 1."
      ],
      "metadata": {
        "id": "ebYC-0zLy_DR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "YBMzBhpjxRRp"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the training data\n",
        "sentences = [['i', 'love', 'you'], ['you', 'are', 'amazing'], ['i', 'hate', 'you']]\n",
        "\n",
        "# Define the vocabulary size and embedding dimension\n",
        "vocab_size = 6\n",
        "embedding_dim = 3\n",
        "\n",
        "# Define the context window size\n",
        "window_size = 1\n",
        "\n",
        "# Vocabulary\n",
        "vocab = []\n",
        "for sentence in sentences:\n",
        "    for word in sentence:\n",
        "        if word not in vocab:\n",
        "            vocab.append(word)\n",
        "\n",
        "\n",
        "# Initialize the weight matrices\n",
        "W_in = np.random.randn(vocab_size, embedding_dim) * 0.01\n",
        "W_out = np.random.randn(embedding_dim, vocab_size) * 0.01\n",
        "\n",
        "def softmax(x):\n",
        "    exp_x = np.exp(x - np.max(x))\n",
        "    return exp_x / np.sum(exp_x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The cbow function implements the CBOW algorithm by iterating over the sentences and for each center word, computing the context vector and the corresponding probability distribution using the softmax function. It then computes the loss and the gradients with respect to W_in and W_out using backpropagation."
      ],
      "metadata": {
        "id": "azez2ENozCRc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the Continuos Bag of words Model\n",
        "def cbow(sentences, W_in, W_out, window_size):\n",
        "    loss = 0.0\n",
        "    for sentence in sentences:\n",
        "        sentence_length = len(sentence)\n",
        "        for i in range(sentence_length):\n",
        "            context_words = []\n",
        "            center_word = sentence[i]\n",
        "            for j in range(max(0, i - window_size), min(sentence_length, i + window_size + 1)):\n",
        "                if j != i:\n",
        "                    context_words.append(sentence[j])\n",
        "            x = np.zeros((1, vocab_size))\n",
        "            for context_word in context_words:\n",
        "                x[0][vocab.index(context_word)] += 1\n",
        "            h = np.dot(x, W_in)\n",
        "            y = np.dot(h, W_out)\n",
        "            p = softmax(y)\n",
        "            loss += -np.log(p[0][vocab.index(center_word)])\n",
        "            dy = p.copy()\n",
        "            dy[0][vocab.index(center_word)] -= 1\n",
        "            dh = np.dot(dy, W_out.T)\n",
        "            dW_out = np.dot(h.T, dy)\n",
        "            dW_in = np.dot(x.T, dh)\n",
        "            W_out -= learning_rate * dW_out\n",
        "            W_in -= learning_rate * dW_in\n",
        "    return loss"
      ],
      "metadata": {
        "id": "Swi9Fak4xct2"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first part of the output shows the loss after each iteration of the training loop. As the number of iterations increases, the loss decreases, indicating that the model is learning to predict the center word given its context."
      ],
      "metadata": {
        "id": "clJBA-Yvzj8h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train cbow\n",
        "learning_rate = 0.01\n",
        "num_iterations = 1000\n",
        "for i in range(num_iterations):\n",
        "    loss = cbow(sentences, W_in, W_out, window_size)\n",
        "    if i % 100 == 0:\n",
        "        print(\"Iteration:\", i, \"Loss:\", loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c3xrckdHx9cJ",
        "outputId": "f080aed5-0467-4c6b-ae9d-ac53a64eb52d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 0 Loss: 16.125790362523496\n",
            "Iteration: 100 Loss: 16.111972333368914\n",
            "Iteration: 200 Loss: 15.37505074664685\n",
            "Iteration: 300 Loss: 10.512859285118665\n",
            "Iteration: 400 Loss: 8.566014166329797\n",
            "Iteration: 500 Loss: 6.7840958880904765\n",
            "Iteration: 600 Loss: 6.158123052838845\n",
            "Iteration: 700 Loss: 5.962475926725965\n",
            "Iteration: 800 Loss: 5.879701123506617\n",
            "Iteration: 900 Loss: 5.836980838732513\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The second part of the output shows the learned embeddings for each word in the vocabulary. These embeddings are the weights of the input matrix W_in and represent the context of each word. We can see that words with similar meanings or contexts, such as \"i\" and \"you\" or \"love\" and \"hate\", have embeddings that are similar in magnitude and direction, while words with different meanings, such as \"amazing\" and \"hate\", have embeddings that are more dissimilar."
      ],
      "metadata": {
        "id": "Qqvu6gtJzRZU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Learned Embeddings:\")\n",
        "for word in ['i', 'love', 'you', 'are', 'amazing', 'hate']:\n",
        "    x = np.zeros((1, vocab_size))\n",
        "    x[0][vocab.index(word)] = 1\n",
        "    h = np.dot(x, W_in)\n",
        "    print(word + \":\", h[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "58a1NYORyGxH",
        "outputId": "3c575c78-22b8-482a-d339-3a5e1044c337"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Learned Embeddings:\n",
            "i: [-1.73109312  0.17126492 -0.435265  ]\n",
            "love: [ 2.07548366  0.49181949 -0.10857048]\n",
            "you: [-0.9239798  -0.94272533  0.62611378]\n",
            "are: [ 0.24049492  1.89290048 -1.28569674]\n",
            "amazing: [ 0.83711837 -1.12707923  1.06111104]\n",
            "hate: [ 2.07798365  0.47997593 -0.12594603]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "CcgNsiWAzzE6"
      }
    }
  ]
}